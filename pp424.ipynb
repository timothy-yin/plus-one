{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (5.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以一筆書目資料試做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
      "Name: Yu-Wei Chang, Affiliation: 1Department of Library and Information Science, National Taiwan University, Taipei, Taiwan\n",
      "Name: Dar-Zen Chen, Affiliation: 3Department of Mechanical Engineering, National Taiwan University, Taipei, Taiwan\n",
      "Name: Mu-Hsuan Huang, Affiliation: 1Department of Library and Information Science, National Taiwan University, Taipei, Taiwan\n",
      "Name: Alberto Baccini, Affiliation: N/A\n",
      "Name: Alberto Baccini, Affiliation: N/A\n",
      "Name: Alberto Baccini, Affiliation: N/A\n",
      "Name: Alberto Baccini, Affiliation: N/A\n"
     ]
    }
   ],
   "source": [
    "# 確保 openpyxl 有安裝（只需執行一次）\n",
    "!pip install openpyxl\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "doi = \"10.1371/journal.pone.0259453\"\n",
    "xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "\n",
    "response = requests.get(xml_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml-xml\")  # 用 XML 解析器\n",
    "\n",
    "authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "\n",
    "records = []\n",
    "\n",
    "# 機構 mapping：aff id -> 機構名稱\n",
    "aff_dict = {}\n",
    "for aff in soup.find_all(\"aff\"):\n",
    "    aff_id = aff.get(\"id\")\n",
    "    aff_text = aff.get_text(strip=True)\n",
    "    aff_dict[aff_id] = aff_text\n",
    "\n",
    "# 解析每位作者\n",
    "for author in authors:\n",
    "    surname = author.find(\"surname\")\n",
    "    given_names = author.find(\"given-names\")\n",
    "    \n",
    "    full_name = \"\"\n",
    "    if given_names and surname:\n",
    "        full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "    elif surname:\n",
    "        full_name = surname.text.strip()\n",
    "\n",
    "    aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "    aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "    affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "    records.append({\"name\": full_name, \"affiliation\": affiliation})\n",
    "\n",
    "# 顯示結果\n",
    "for r in records:\n",
    "    print(f\"Name: {r['name']}, Affiliation: {r['affiliation']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 輸出成excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已成功儲存 Excel 檔案於：/var/folders/8p/k5cclwt549dg7s3z944xq9h80000gn/T/tmpw4r11pvd.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 設定 DOI\n",
    "doi = \"10.1371/journal.pone.0259453\"\n",
    "xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "\n",
    "# 發送請求並解析 XML\n",
    "response = requests.get(xml_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml-xml\")  # 用 XML 解析\n",
    "\n",
    "# 準備機構 mapping：aff id -> 機構名稱\n",
    "aff_dict = {}\n",
    "for aff in soup.find_all(\"aff\"):\n",
    "    aff_id = aff.get(\"id\")\n",
    "    aff_text = aff.get_text(strip=True)\n",
    "    aff_dict[aff_id] = aff_text\n",
    "\n",
    "# 擷取作者資訊\n",
    "records = []\n",
    "authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "for author in authors:\n",
    "    surname = author.find(\"surname\")\n",
    "    given_names = author.find(\"given-names\")\n",
    "    \n",
    "    if given_names and surname:\n",
    "        full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "    elif surname:\n",
    "        full_name = surname.text.strip()\n",
    "    else:\n",
    "        full_name = \"N/A\"\n",
    "\n",
    "    # 找作者對應的機構 ID 與實際名稱\n",
    "    aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "    aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "    affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "    records.append({\"Name\": full_name, \"Affiliation\": affiliation})\n",
    "\n",
    "# 存成 DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# 儲存為 Excel\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "    output_path = tmpfile.name\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"✅ 已成功儲存 Excel 檔案於：{output_path}\")\n",
    "# ✅ 自動打開檔案（限 macOS 有安裝 Excel 的狀況）\n",
    "os.system(f\"open {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已成功儲存 Excel 檔案於：/var/folders/8p/k5cclwt549dg7s3z944xq9h80000gn/T/tmpwfi4n7u9.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# === Step 1: 設定 DOI ===\n",
    "doi = \"10.1371/journal.pone.0259453\"\n",
    "xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "\n",
    "# === Step 2: 發送請求並解析 XML ===\n",
    "response = requests.get(xml_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "\n",
    "# === Step 3: 機構 mapping：aff id -> 機構名稱 ===\n",
    "aff_dict = {}\n",
    "for aff in soup.find_all(\"aff\"):\n",
    "    aff_id = aff.get(\"id\")\n",
    "    aff_text = aff.get_text(strip=True)\n",
    "    aff_dict[aff_id] = aff_text\n",
    "\n",
    "# === Step 4: 擷取作者資訊 ===\n",
    "records = []\n",
    "authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "for author in authors:\n",
    "    surname = author.find(\"surname\")\n",
    "    given_names = author.find(\"given-names\")\n",
    "    \n",
    "    if given_names and surname:\n",
    "        full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "    elif surname:\n",
    "        full_name = surname.text.strip()\n",
    "    else:\n",
    "        full_name = \"N/A\"\n",
    "\n",
    "    # 機構\n",
    "    aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "    aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "    affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "    # 是否來自台灣？\n",
    "    is_taiwan = \"taiwan\" in affiliation.lower()\n",
    "\n",
    "    records.append({\n",
    "        \"Name\": full_name,\n",
    "        \"Affiliation\": affiliation,\n",
    "        \"Is_Taiwan_Affiliation\": is_taiwan\n",
    "    })\n",
    "\n",
    "# === Step 5: 存成 Excel ===\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "    output_path = tmpfile.name\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"✅ 已成功儲存 Excel 檔案於：{output_path}\")\n",
    "\n",
    "# 若在 macOS 上可自動打開\n",
    "os.system(f\"open {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "doi = \"10.1371/journal.pone.0170929\"\n",
    "url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一次處理三篇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已成功儲存 Excel 檔案於：/var/folders/8p/k5cclwt549dg7s3z944xq9h80000gn/T/tmpq618i_ry.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 定義要查詢的 DOI 列表\n",
    "dois = [\n",
    "    \"10.1371/journal.pone.0259453\",\n",
    "    \"10.1371/journal.pone.0260961\",\n",
    "    \"10.1371/journal.pone.0274826\"\n",
    "    # 在此處繼續添加更多 DOI\n",
    "]\n",
    "\n",
    "# 建立一個空的列表來儲存資料\n",
    "all_records = []\n",
    "\n",
    "# 依序處理每個 DOI\n",
    "for doi in dois:\n",
    "    xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "    response = requests.get(xml_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "        \n",
    "        # 準備機構映射：aff id -> 機構名稱\n",
    "        aff_dict = {}\n",
    "        for aff in soup.find_all(\"aff\"):\n",
    "            aff_id = aff.get(\"id\")\n",
    "            aff_text = aff.get_text(strip=True)\n",
    "            aff_dict[aff_id] = aff_text\n",
    "\n",
    "        # 擷取作者資訊\n",
    "        authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "        for author in authors:\n",
    "            surname = author.find(\"surname\")\n",
    "            given_names = author.find(\"given-names\")\n",
    "            \n",
    "            if given_names and surname:\n",
    "                full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "            elif surname:\n",
    "                full_name = surname.text.strip()\n",
    "            else:\n",
    "                full_name = \"N/A\"\n",
    "            \n",
    "            # 找作者對應的機構 ID 與實際名稱\n",
    "            aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "            aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "            affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "            all_records.append({\"DOI\": doi, \"Name\": full_name, \"Affiliation\": affiliation})\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for DOI {doi}, status code: {response.status_code}\")\n",
    "\n",
    "# 儲存結果為 DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "    output_path = tmpfile.name\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"✅ 已成功儲存 Excel 檔案於：{output_path}\")\n",
    "\n",
    "# 若在 macOS 上可自動打開\n",
    "os.system(f\"open {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (5.5.2)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.2.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (11.1.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-19.0.1-cp313-cp313-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (4.13.1)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.36.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m152.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m133.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pyarrow-19.0.1-cp313-cp313-macosx_12_0_x86_64.whl (32.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.1/32.1 MB\u001b[0m \u001b[31m258.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m195.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading narwhals-1.36.0-py3-none-any.whl (331 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, pyarrow, protobuf, narwhals, click, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.30.2\n",
      "    Uninstalling protobuf-6.30.2:\n",
      "      Successfully uninstalled protobuf-6.30.2\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 narwhals-1.36.0 protobuf-5.29.4 pyarrow-19.0.1 pydeck-0.9.1 smmap-5.0.2 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:12:53.024 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.026 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.152 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/timothy/Library/Python/3.13/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-24 16:12:53.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.159 Session state does not function when running a script without `streamlit run`\n",
      "2025-04-24 16:12:53.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "# 擷取資料的主函式\n",
    "def fetch_metadata_from_dois(dois):\n",
    "    all_records = []\n",
    "    for doi in dois:\n",
    "        xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "        response = requests.get(xml_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "            aff_dict = {aff.get(\"id\"): aff.get_text(strip=True) for aff in soup.find_all(\"aff\")}\n",
    "            authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "\n",
    "            for author in authors:\n",
    "                surname = author.find(\"surname\")\n",
    "                given_names = author.find(\"given-names\")\n",
    "                full_name = f\"{given_names.text.strip()} {surname.text.strip()}\" if given_names and surname else (surname.text.strip() if surname else \"N/A\")\n",
    "                aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "                aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "                affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "                all_records.append({\"DOI\": doi, \"Name\": full_name, \"Affiliation\": affiliation})\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "# 建立下載連結\n",
    "def generate_excel_download_link(df):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "        df.to_excel(tmpfile.name, index=False, engine=\"openpyxl\")\n",
    "        tmpfile_path = tmpfile.name\n",
    "\n",
    "    with open(tmpfile_path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode()\n",
    "    href = f'<a href=\"data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}\" download=\"doi_affiliations.xlsx\">📥 下載 Excel 檔案</a>'\n",
    "    return href\n",
    "\n",
    "# Streamlit UI\n",
    "st.set_page_config(page_title=\"DOI 作者與機構擷取器\", layout=\"wide\")\n",
    "st.title(\"🔍 DOI 作者與機構擷取器\")\n",
    "st.markdown(\"輸入 PLOS DOI 列表（每行一筆），擷取作者姓名與所屬機構資訊：\")\n",
    "\n",
    "input_text = st.text_area(\"請輸入 DOI（每行一筆）\", height=200)\n",
    "\n",
    "if st.button(\"🚀 開始擷取資料\"):\n",
    "    doi_list = [line.strip() for line in input_text.splitlines() if line.strip()]\n",
    "    if doi_list:\n",
    "        df = fetch_metadata_from_dois(doi_list)\n",
    "        st.success(f\"✅ 共擷取 {len(df)} 筆作者資料\")\n",
    "        st.dataframe(df)\n",
    "        st.markdown(generate_excel_download_link(df), unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.warning(\"請至少輸入一筆 DOI。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:15:29.290 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.292 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.302 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.304 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.307 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "st.title(\"📚 DOI 作者與機構擷取工具\")\n",
    "\n",
    "doi_input = st.text_area(\"請輸入 DOI（每行一筆）\")\n",
    "run_button = st.button(\"🚀 開始擷取\")\n",
    "\n",
    "if run_button and doi_input.strip():\n",
    "    dois = [d.strip() for d in doi_input.strip().split(\"\\n\")]\n",
    "    all_records = []\n",
    "\n",
    "    for doi in dois:\n",
    "        xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "        response = requests.get(xml_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "            aff_dict = {aff.get(\"id\"): aff.get_text(strip=True) for aff in soup.find_all(\"aff\")}\n",
    "            authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "\n",
    "            for author in authors:\n",
    "                surname = author.find(\"surname\")\n",
    "                given_names = author.find(\"given-names\")\n",
    "                name = f\"{given_names.text.strip()} {surname.text.strip()}\" if given_names and surname else surname.text.strip() if surname else \"N/A\"\n",
    "                aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "                aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "                affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "                all_records.append({\"DOI\": doi, \"Name\": name, \"Affiliation\": affiliation})\n",
    "        else:\n",
    "            st.warning(f\"❌ 無法取得 DOI: {doi}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmp:\n",
    "        df.to_excel(tmp.name, index=False)\n",
    "        st.success(\"✅ 擷取完成！\")\n",
    "        st.download_button(\"⬇️ 下載 Excel\", data=open(tmp.name, 'rb'), file_name=\"authors_affiliations.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬蟲（尚未成功，這禮拜考完期中考再做更詳細處理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始呼叫 PLOS API...\n",
      "取得 0 篇文章。\n",
      "結果 DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "✅ 成功將結果儲存為 plos_taiwan_affiliations.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: 呼叫 PLOS 搜尋 API（方式一：利用全文文字搜尋）\n",
    "# ------------------------------\n",
    "base_url = \"http://api.plos.org/search\"\n",
    "params = {\n",
    "    \"q\": \"journal:PLoSONE\",  # 改成只篩 PLOS ONE\n",
    "    \"fl\": \"id,title_display\",            # 回傳欄位，id 為 DOI\n",
    "    \"wt\": \"json\",\n",
    "    \"rows\": 100,                         # 取 100 筆\n",
    "    \"start\": 0\n",
    "}\n",
    "\n",
    "print(\"開始呼叫 PLOS API...\")\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# 若 API 回傳結構不同，請檢查 data 結構\n",
    "docs = data.get(\"response\", {}).get(\"docs\", [])\n",
    "print(f\"取得 {len(docs)} 篇文章。\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: 逐篇文章抓取 JATS XML，並解析機構資訊\n",
    "# ------------------------------\n",
    "records = []\n",
    "\n",
    "# 這個函數從一篇文章的 XML 中找出所有包含 \"Taiwan\" 的機構資訊\n",
    "def extract_taiwan_affiliations(xml_content, doi, title):\n",
    "    # 使用 lxml-xml 解析 XML\n",
    "    soup = BeautifulSoup(xml_content, \"lxml-xml\")\n",
    "    for aff in soup.find_all(\"aff\"):\n",
    "        # 取得機構文字，使用空格將內部 tag 合併\n",
    "        aff_text = aff.get_text(separator=\" \", strip=True)\n",
    "        if \"Taiwan\" in aff_text:\n",
    "            records.append({\n",
    "                \"doi\": doi,\n",
    "                \"title\": title,\n",
    "                \"affiliation\": aff_text\n",
    "            })\n",
    "\n",
    "# 針對取得的每篇文章進行處理\n",
    "for doc in docs:\n",
    "    doi = doc.get(\"id\")\n",
    "    title = doc.get(\"title_display\", \"\")\n",
    "    # 組成 JATS XML 的下載 URL\n",
    "    xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "    try:\n",
    "        xml_response = requests.get(xml_url)\n",
    "        # 簡單延遲，避免過快請求\n",
    "        time.sleep(0.1)\n",
    "        if xml_response.status_code == 200:\n",
    "            extract_taiwan_affiliations(xml_response.content, doi, title)\n",
    "        else:\n",
    "            print(f\"doi: {doi} 無法取得 XML (狀態碼: {xml_response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"doi: {doi} 取得或解析失敗：{e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: 將結果存成 DataFrame 並輸出\n",
    "# ------------------------------\n",
    "df = pd.DataFrame(records)\n",
    "print(\"結果 DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# 若需要存成 Excel 檔（例如在 Jupyter Notebook 本機可寫入檔案系統）\n",
    "output_file = \"plos_taiwan_affiliations.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"✅ 成功將結果儲存為 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 開始從 dynamicSearch API 抓取 DOI ...\n",
      "❌ 第 464 頁請求失敗：('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "🎯 共抓到 0 筆 DOI\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_dois_from_dynamic_search(max_results=10):\n",
    "    base_url = \"https://journals.plos.org/plosone/dynamicSearch\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    page = 1\n",
    "    dois = []\n",
    "\n",
    "    print(\"🔍 開始從 dynamicSearch API 抓取 DOI ...\")\n",
    "\n",
    "    while len(dois) < max_results:\n",
    "        params = {\n",
    "            \"q\": \"author_affiliate:taiwan\",\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ 第 {page} 頁請求失敗：{e}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        results = data.get(\"searchResults\", [])\n",
    "        if not results:\n",
    "            print(f\"⚠️ 第 {page} 頁無結果\")\n",
    "            break\n",
    "\n",
    "        for html_snippet in results:\n",
    "            match = re.search(r'https://doi.org/(10\\.1371/journal\\.pone\\.\\d+)', html_snippet)\n",
    "            if match:\n",
    "                doi = match.group(1)\n",
    "                if doi not in dois:\n",
    "                    dois.append(doi)\n",
    "                    print(f\"✅ 抓到 DOI: {doi}\")\n",
    "                    if len(dois) >= max_results:\n",
    "                        break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)  # 每頁等待 1 秒，避免被封鎖\n",
    "\n",
    "    print(f\"🎯 共抓到 {len(dois)} 筆 DOI\")\n",
    "    return dois\n",
    "\n",
    "# 測試前 10 篇\n",
    "dois = get_dois_from_dynamic_search(max_results=10)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
