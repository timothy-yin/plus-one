{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (5.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»¥ä¸€ç­†æ›¸ç›®è³‡æ–™è©¦åš"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
      "Name: Yu-Wei Chang, Affiliation: 1Department of Library and Information Science, National Taiwan University, Taipei, Taiwan\n",
      "Name: Dar-Zen Chen, Affiliation: 3Department of Mechanical Engineering, National Taiwan University, Taipei, Taiwan\n",
      "Name: Mu-Hsuan Huang, Affiliation: 1Department of Library and Information Science, National Taiwan University, Taipei, Taiwan\n",
      "Name: Alberto Baccini, Affiliation: N/A\n",
      "Name: Alberto Baccini, Affiliation: N/A\n",
      "Name: Alberto Baccini, Affiliation: N/A\n",
      "Name: Alberto Baccini, Affiliation: N/A\n"
     ]
    }
   ],
   "source": [
    "# ç¢ºä¿ openpyxl æœ‰å®‰è£ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼‰\n",
    "!pip install openpyxl\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "doi = \"10.1371/journal.pone.0259453\"\n",
    "xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "\n",
    "response = requests.get(xml_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml-xml\")  # ç”¨ XML è§£æå™¨\n",
    "\n",
    "authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "\n",
    "records = []\n",
    "\n",
    "# æ©Ÿæ§‹ mappingï¼šaff id -> æ©Ÿæ§‹åç¨±\n",
    "aff_dict = {}\n",
    "for aff in soup.find_all(\"aff\"):\n",
    "    aff_id = aff.get(\"id\")\n",
    "    aff_text = aff.get_text(strip=True)\n",
    "    aff_dict[aff_id] = aff_text\n",
    "\n",
    "# è§£ææ¯ä½ä½œè€…\n",
    "for author in authors:\n",
    "    surname = author.find(\"surname\")\n",
    "    given_names = author.find(\"given-names\")\n",
    "    \n",
    "    full_name = \"\"\n",
    "    if given_names and surname:\n",
    "        full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "    elif surname:\n",
    "        full_name = surname.text.strip()\n",
    "\n",
    "    aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "    aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "    affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "    records.append({\"name\": full_name, \"affiliation\": affiliation})\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "for r in records:\n",
    "    print(f\"Name: {r['name']}, Affiliation: {r['affiliation']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¼¸å‡ºæˆexcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æˆåŠŸå„²å­˜ Excel æª”æ¡ˆæ–¼ï¼š/var/folders/8p/k5cclwt549dg7s3z944xq9h80000gn/T/tmpw4r11pvd.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# è¨­å®š DOI\n",
    "doi = \"10.1371/journal.pone.0259453\"\n",
    "xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "\n",
    "# ç™¼é€è«‹æ±‚ä¸¦è§£æ XML\n",
    "response = requests.get(xml_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml-xml\")  # ç”¨ XML è§£æ\n",
    "\n",
    "# æº–å‚™æ©Ÿæ§‹ mappingï¼šaff id -> æ©Ÿæ§‹åç¨±\n",
    "aff_dict = {}\n",
    "for aff in soup.find_all(\"aff\"):\n",
    "    aff_id = aff.get(\"id\")\n",
    "    aff_text = aff.get_text(strip=True)\n",
    "    aff_dict[aff_id] = aff_text\n",
    "\n",
    "# æ“·å–ä½œè€…è³‡è¨Š\n",
    "records = []\n",
    "authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "for author in authors:\n",
    "    surname = author.find(\"surname\")\n",
    "    given_names = author.find(\"given-names\")\n",
    "    \n",
    "    if given_names and surname:\n",
    "        full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "    elif surname:\n",
    "        full_name = surname.text.strip()\n",
    "    else:\n",
    "        full_name = \"N/A\"\n",
    "\n",
    "    # æ‰¾ä½œè€…å°æ‡‰çš„æ©Ÿæ§‹ ID èˆ‡å¯¦éš›åç¨±\n",
    "    aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "    aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "    affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "    records.append({\"Name\": full_name, \"Affiliation\": affiliation})\n",
    "\n",
    "# å­˜æˆ DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# å„²å­˜ç‚º Excel\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "    output_path = tmpfile.name\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"âœ… å·²æˆåŠŸå„²å­˜ Excel æª”æ¡ˆæ–¼ï¼š{output_path}\")\n",
    "# âœ… è‡ªå‹•æ‰“é–‹æª”æ¡ˆï¼ˆé™ macOS æœ‰å®‰è£ Excel çš„ç‹€æ³ï¼‰\n",
    "os.system(f\"open {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æˆåŠŸå„²å­˜ Excel æª”æ¡ˆæ–¼ï¼š/var/folders/8p/k5cclwt549dg7s3z944xq9h80000gn/T/tmpwfi4n7u9.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# === Step 1: è¨­å®š DOI ===\n",
    "doi = \"10.1371/journal.pone.0259453\"\n",
    "xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "\n",
    "# === Step 2: ç™¼é€è«‹æ±‚ä¸¦è§£æ XML ===\n",
    "response = requests.get(xml_url)\n",
    "soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "\n",
    "# === Step 3: æ©Ÿæ§‹ mappingï¼šaff id -> æ©Ÿæ§‹åç¨± ===\n",
    "aff_dict = {}\n",
    "for aff in soup.find_all(\"aff\"):\n",
    "    aff_id = aff.get(\"id\")\n",
    "    aff_text = aff.get_text(strip=True)\n",
    "    aff_dict[aff_id] = aff_text\n",
    "\n",
    "# === Step 4: æ“·å–ä½œè€…è³‡è¨Š ===\n",
    "records = []\n",
    "authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "for author in authors:\n",
    "    surname = author.find(\"surname\")\n",
    "    given_names = author.find(\"given-names\")\n",
    "    \n",
    "    if given_names and surname:\n",
    "        full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "    elif surname:\n",
    "        full_name = surname.text.strip()\n",
    "    else:\n",
    "        full_name = \"N/A\"\n",
    "\n",
    "    # æ©Ÿæ§‹\n",
    "    aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "    aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "    affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "    # æ˜¯å¦ä¾†è‡ªå°ç£ï¼Ÿ\n",
    "    is_taiwan = \"taiwan\" in affiliation.lower()\n",
    "\n",
    "    records.append({\n",
    "        \"Name\": full_name,\n",
    "        \"Affiliation\": affiliation,\n",
    "        \"Is_Taiwan_Affiliation\": is_taiwan\n",
    "    })\n",
    "\n",
    "# === Step 5: å­˜æˆ Excel ===\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "    output_path = tmpfile.name\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"âœ… å·²æˆåŠŸå„²å­˜ Excel æª”æ¡ˆæ–¼ï¼š{output_path}\")\n",
    "\n",
    "# è‹¥åœ¨ macOS ä¸Šå¯è‡ªå‹•æ‰“é–‹\n",
    "os.system(f\"open {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "doi = \"10.1371/journal.pone.0170929\"\n",
    "url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "response = requests.get(url)\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¸€æ¬¡è™•ç†ä¸‰ç¯‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æˆåŠŸå„²å­˜ Excel æª”æ¡ˆæ–¼ï¼š/var/folders/8p/k5cclwt549dg7s3z944xq9h80000gn/T/tmpq618i_ry.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# å®šç¾©è¦æŸ¥è©¢çš„ DOI åˆ—è¡¨\n",
    "dois = [\n",
    "    \"10.1371/journal.pone.0259453\",\n",
    "    \"10.1371/journal.pone.0260961\",\n",
    "    \"10.1371/journal.pone.0274826\"\n",
    "    # åœ¨æ­¤è™•ç¹¼çºŒæ·»åŠ æ›´å¤š DOI\n",
    "]\n",
    "\n",
    "# å»ºç«‹ä¸€å€‹ç©ºçš„åˆ—è¡¨ä¾†å„²å­˜è³‡æ–™\n",
    "all_records = []\n",
    "\n",
    "# ä¾åºè™•ç†æ¯å€‹ DOI\n",
    "for doi in dois:\n",
    "    xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "    response = requests.get(xml_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "        \n",
    "        # æº–å‚™æ©Ÿæ§‹æ˜ å°„ï¼šaff id -> æ©Ÿæ§‹åç¨±\n",
    "        aff_dict = {}\n",
    "        for aff in soup.find_all(\"aff\"):\n",
    "            aff_id = aff.get(\"id\")\n",
    "            aff_text = aff.get_text(strip=True)\n",
    "            aff_dict[aff_id] = aff_text\n",
    "\n",
    "        # æ“·å–ä½œè€…è³‡è¨Š\n",
    "        authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "        for author in authors:\n",
    "            surname = author.find(\"surname\")\n",
    "            given_names = author.find(\"given-names\")\n",
    "            \n",
    "            if given_names and surname:\n",
    "                full_name = f\"{given_names.text.strip()} {surname.text.strip()}\"\n",
    "            elif surname:\n",
    "                full_name = surname.text.strip()\n",
    "            else:\n",
    "                full_name = \"N/A\"\n",
    "            \n",
    "            # æ‰¾ä½œè€…å°æ‡‰çš„æ©Ÿæ§‹ ID èˆ‡å¯¦éš›åç¨±\n",
    "            aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "            aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "            affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "            all_records.append({\"DOI\": doi, \"Name\": full_name, \"Affiliation\": affiliation})\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for DOI {doi}, status code: {response.status_code}\")\n",
    "\n",
    "# å„²å­˜çµæœç‚º DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "    output_path = tmpfile.name\n",
    "    df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"âœ… å·²æˆåŠŸå„²å­˜ Excel æª”æ¡ˆæ–¼ï¼š{output_path}\")\n",
    "\n",
    "# è‹¥åœ¨ macOS ä¸Šå¯è‡ªå‹•æ‰“é–‹\n",
    "os.system(f\"open {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (5.5.2)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.2.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (11.1.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-19.0.1-cp313-cp313-macosx_12_0_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from streamlit) (4.13.1)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.36.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/timothy/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m152.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m133.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pyarrow-19.0.1-cp313-cp313-macosx_12_0_x86_64.whl (32.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m32.1/32.1 MB\u001b[0m \u001b[31m258.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m195.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading narwhals-1.36.0-py3-none-any.whl (331 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, pyarrow, protobuf, narwhals, click, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.30.2\n",
      "    Uninstalling protobuf-6.30.2:\n",
      "      Successfully uninstalled protobuf-6.30.2\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 narwhals-1.36.0 protobuf-5.29.4 pyarrow-19.0.1 pydeck-0.9.1 smmap-5.0.2 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:12:53.024 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.026 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.152 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/timothy/Library/Python/3.13/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-24 16:12:53.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.159 Session state does not function when running a script without `streamlit run`\n",
      "2025-04-24 16:12:53.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:12:53.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n",
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import base64\n",
    "\n",
    "# æ“·å–è³‡æ–™çš„ä¸»å‡½å¼\n",
    "def fetch_metadata_from_dois(dois):\n",
    "    all_records = []\n",
    "    for doi in dois:\n",
    "        xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "        response = requests.get(xml_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "            aff_dict = {aff.get(\"id\"): aff.get_text(strip=True) for aff in soup.find_all(\"aff\")}\n",
    "            authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "\n",
    "            for author in authors:\n",
    "                surname = author.find(\"surname\")\n",
    "                given_names = author.find(\"given-names\")\n",
    "                full_name = f\"{given_names.text.strip()} {surname.text.strip()}\" if given_names and surname else (surname.text.strip() if surname else \"N/A\")\n",
    "                aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "                aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "                affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "\n",
    "                all_records.append({\"DOI\": doi, \"Name\": full_name, \"Affiliation\": affiliation})\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "# å»ºç«‹ä¸‹è¼‰é€£çµ\n",
    "def generate_excel_download_link(df):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmpfile:\n",
    "        df.to_excel(tmpfile.name, index=False, engine=\"openpyxl\")\n",
    "        tmpfile_path = tmpfile.name\n",
    "\n",
    "    with open(tmpfile_path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode()\n",
    "    href = f'<a href=\"data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}\" download=\"doi_affiliations.xlsx\">ğŸ“¥ ä¸‹è¼‰ Excel æª”æ¡ˆ</a>'\n",
    "    return href\n",
    "\n",
    "# Streamlit UI\n",
    "st.set_page_config(page_title=\"DOI ä½œè€…èˆ‡æ©Ÿæ§‹æ“·å–å™¨\", layout=\"wide\")\n",
    "st.title(\"ğŸ” DOI ä½œè€…èˆ‡æ©Ÿæ§‹æ“·å–å™¨\")\n",
    "st.markdown(\"è¼¸å…¥ PLOS DOI åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ç­†ï¼‰ï¼Œæ“·å–ä½œè€…å§“åèˆ‡æ‰€å±¬æ©Ÿæ§‹è³‡è¨Šï¼š\")\n",
    "\n",
    "input_text = st.text_area(\"è«‹è¼¸å…¥ DOIï¼ˆæ¯è¡Œä¸€ç­†ï¼‰\", height=200)\n",
    "\n",
    "if st.button(\"ğŸš€ é–‹å§‹æ“·å–è³‡æ–™\"):\n",
    "    doi_list = [line.strip() for line in input_text.splitlines() if line.strip()]\n",
    "    if doi_list:\n",
    "        df = fetch_metadata_from_dois(doi_list)\n",
    "        st.success(f\"âœ… å…±æ“·å– {len(df)} ç­†ä½œè€…è³‡æ–™\")\n",
    "        st.dataframe(df)\n",
    "        st.markdown(generate_excel_download_link(df), unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.warning(\"è«‹è‡³å°‘è¼¸å…¥ä¸€ç­† DOIã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 16:15:29.290 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.292 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.302 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.304 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-24 16:15:29.307 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "st.title(\"ğŸ“š DOI ä½œè€…èˆ‡æ©Ÿæ§‹æ“·å–å·¥å…·\")\n",
    "\n",
    "doi_input = st.text_area(\"è«‹è¼¸å…¥ DOIï¼ˆæ¯è¡Œä¸€ç­†ï¼‰\")\n",
    "run_button = st.button(\"ğŸš€ é–‹å§‹æ“·å–\")\n",
    "\n",
    "if run_button and doi_input.strip():\n",
    "    dois = [d.strip() for d in doi_input.strip().split(\"\\n\")]\n",
    "    all_records = []\n",
    "\n",
    "    for doi in dois:\n",
    "        xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "        response = requests.get(xml_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"lxml-xml\")\n",
    "            aff_dict = {aff.get(\"id\"): aff.get_text(strip=True) for aff in soup.find_all(\"aff\")}\n",
    "            authors = soup.find_all(\"contrib\", {\"contrib-type\": \"author\"})\n",
    "\n",
    "            for author in authors:\n",
    "                surname = author.find(\"surname\")\n",
    "                given_names = author.find(\"given-names\")\n",
    "                name = f\"{given_names.text.strip()} {surname.text.strip()}\" if given_names and surname else surname.text.strip() if surname else \"N/A\"\n",
    "                aff_ref = author.find(\"xref\", {\"ref-type\": \"aff\"})\n",
    "                aff_id = aff_ref.get(\"rid\") if aff_ref else None\n",
    "                affiliation = aff_dict.get(aff_id, \"N/A\")\n",
    "                all_records.append({\"DOI\": doi, \"Name\": name, \"Affiliation\": affiliation})\n",
    "        else:\n",
    "            st.warning(f\"âŒ ç„¡æ³•å–å¾— DOI: {doi}\")\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmp:\n",
    "        df.to_excel(tmp.name, index=False)\n",
    "        st.success(\"âœ… æ“·å–å®Œæˆï¼\")\n",
    "        st.download_button(\"â¬‡ï¸ ä¸‹è¼‰ Excel\", data=open(tmp.name, 'rb'), file_name=\"authors_affiliations.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# çˆ¬èŸ²ï¼ˆå°šæœªæˆåŠŸï¼Œé€™ç¦®æ‹œè€ƒå®ŒæœŸä¸­è€ƒå†åšæ›´è©³ç´°è™•ç†ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é–‹å§‹å‘¼å« PLOS API...\n",
      "å–å¾— 0 ç¯‡æ–‡ç« ã€‚\n",
      "çµæœ DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "âœ… æˆåŠŸå°‡çµæœå„²å­˜ç‚º plos_taiwan_affiliations.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: å‘¼å« PLOS æœå°‹ APIï¼ˆæ–¹å¼ä¸€ï¼šåˆ©ç”¨å…¨æ–‡æ–‡å­—æœå°‹ï¼‰\n",
    "# ------------------------------\n",
    "base_url = \"http://api.plos.org/search\"\n",
    "params = {\n",
    "    \"q\": \"journal:PLoSONE\",  # æ”¹æˆåªç¯© PLOS ONE\n",
    "    \"fl\": \"id,title_display\",            # å›å‚³æ¬„ä½ï¼Œid ç‚º DOI\n",
    "    \"wt\": \"json\",\n",
    "    \"rows\": 100,                         # å– 100 ç­†\n",
    "    \"start\": 0\n",
    "}\n",
    "\n",
    "print(\"é–‹å§‹å‘¼å« PLOS API...\")\n",
    "response = requests.get(base_url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# è‹¥ API å›å‚³çµæ§‹ä¸åŒï¼Œè«‹æª¢æŸ¥ data çµæ§‹\n",
    "docs = data.get(\"response\", {}).get(\"docs\", [])\n",
    "print(f\"å–å¾— {len(docs)} ç¯‡æ–‡ç« ã€‚\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: é€ç¯‡æ–‡ç« æŠ“å– JATS XMLï¼Œä¸¦è§£ææ©Ÿæ§‹è³‡è¨Š\n",
    "# ------------------------------\n",
    "records = []\n",
    "\n",
    "# é€™å€‹å‡½æ•¸å¾ä¸€ç¯‡æ–‡ç« çš„ XML ä¸­æ‰¾å‡ºæ‰€æœ‰åŒ…å« \"Taiwan\" çš„æ©Ÿæ§‹è³‡è¨Š\n",
    "def extract_taiwan_affiliations(xml_content, doi, title):\n",
    "    # ä½¿ç”¨ lxml-xml è§£æ XML\n",
    "    soup = BeautifulSoup(xml_content, \"lxml-xml\")\n",
    "    for aff in soup.find_all(\"aff\"):\n",
    "        # å–å¾—æ©Ÿæ§‹æ–‡å­—ï¼Œä½¿ç”¨ç©ºæ ¼å°‡å…§éƒ¨ tag åˆä½µ\n",
    "        aff_text = aff.get_text(separator=\" \", strip=True)\n",
    "        if \"Taiwan\" in aff_text:\n",
    "            records.append({\n",
    "                \"doi\": doi,\n",
    "                \"title\": title,\n",
    "                \"affiliation\": aff_text\n",
    "            })\n",
    "\n",
    "# é‡å°å–å¾—çš„æ¯ç¯‡æ–‡ç« é€²è¡Œè™•ç†\n",
    "for doc in docs:\n",
    "    doi = doc.get(\"id\")\n",
    "    title = doc.get(\"title_display\", \"\")\n",
    "    # çµ„æˆ JATS XML çš„ä¸‹è¼‰ URL\n",
    "    xml_url = f\"https://journals.plos.org/plosone/article/file?id={doi}&type=manuscript\"\n",
    "    try:\n",
    "        xml_response = requests.get(xml_url)\n",
    "        # ç°¡å–®å»¶é²ï¼Œé¿å…éå¿«è«‹æ±‚\n",
    "        time.sleep(0.1)\n",
    "        if xml_response.status_code == 200:\n",
    "            extract_taiwan_affiliations(xml_response.content, doi, title)\n",
    "        else:\n",
    "            print(f\"doi: {doi} ç„¡æ³•å–å¾— XML (ç‹€æ…‹ç¢¼: {xml_response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"doi: {doi} å–å¾—æˆ–è§£æå¤±æ•—ï¼š{e}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: å°‡çµæœå­˜æˆ DataFrame ä¸¦è¼¸å‡º\n",
    "# ------------------------------\n",
    "df = pd.DataFrame(records)\n",
    "print(\"çµæœ DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# è‹¥éœ€è¦å­˜æˆ Excel æª”ï¼ˆä¾‹å¦‚åœ¨ Jupyter Notebook æœ¬æ©Ÿå¯å¯«å…¥æª”æ¡ˆç³»çµ±ï¼‰\n",
    "output_file = \"plos_taiwan_affiliations.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "print(f\"âœ… æˆåŠŸå°‡çµæœå„²å­˜ç‚º {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” é–‹å§‹å¾ dynamicSearch API æŠ“å– DOI ...\n",
      "âŒ ç¬¬ 464 é è«‹æ±‚å¤±æ•—ï¼š('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "ğŸ¯ å…±æŠ“åˆ° 0 ç­† DOI\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_dois_from_dynamic_search(max_results=10):\n",
    "    base_url = \"https://journals.plos.org/plosone/dynamicSearch\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    page = 1\n",
    "    dois = []\n",
    "\n",
    "    print(\"ğŸ” é–‹å§‹å¾ dynamicSearch API æŠ“å– DOI ...\")\n",
    "\n",
    "    while len(dois) < max_results:\n",
    "        params = {\n",
    "            \"q\": \"author_affiliate:taiwan\",\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ ç¬¬ {page} é è«‹æ±‚å¤±æ•—ï¼š{e}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        results = data.get(\"searchResults\", [])\n",
    "        if not results:\n",
    "            print(f\"âš ï¸ ç¬¬ {page} é ç„¡çµæœ\")\n",
    "            break\n",
    "\n",
    "        for html_snippet in results:\n",
    "            match = re.search(r'https://doi.org/(10\\.1371/journal\\.pone\\.\\d+)', html_snippet)\n",
    "            if match:\n",
    "                doi = match.group(1)\n",
    "                if doi not in dois:\n",
    "                    dois.append(doi)\n",
    "                    print(f\"âœ… æŠ“åˆ° DOI: {doi}\")\n",
    "                    if len(dois) >= max_results:\n",
    "                        break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(1)  # æ¯é ç­‰å¾… 1 ç§’ï¼Œé¿å…è¢«å°é–\n",
    "\n",
    "    print(f\"ğŸ¯ å…±æŠ“åˆ° {len(dois)} ç­† DOI\")\n",
    "    return dois\n",
    "\n",
    "# æ¸¬è©¦å‰ 10 ç¯‡\n",
    "dois = get_dois_from_dynamic_search(max_results=10)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
